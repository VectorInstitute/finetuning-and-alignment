{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import datasets\n",
    "\n",
    "# \"dictionary\" mapping name of split (train/validation/test) to\n",
    "# a Dataset for that split.\n",
    "dataset_dict = datasets.load_dataset(\"tweet_eval\", \"emotion\")\n",
    "\n",
    "example_row = dataset_dict[\"train\"][1]\n",
    "print(\"Dataset overview:\", dataset_dict)\n",
    "print(\"Dataset features:\", example_row.keys())\n",
    "print(\"Example row:\", example_row)\n",
    "print()\n",
    "\n",
    "label_distribution: dict[str, Counter] = {\n",
    "    split_name: Counter([row[\"label\"] for row in split_dataset])\n",
    "    for split_name, split_dataset in dataset_dict.items()\n",
    "}\n",
    "num_label_classes = len(label_distribution[\"train\"].keys())\n",
    "print(\"Number of classes:\", num_label_classes)\n",
    "for split_name, split_label_distribution in label_distribution.items():\n",
    "    print(f'Label distribution for \"{split_name}\" split:', split_label_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "# Use shared copy of the model if running this example on the Vector cluster.\n",
    "# Otherwise, download model from HuggingFace.\n",
    "base_model_repo = \"facebook/opt-350m\"\n",
    "local_model_path = \"/projects/fta_bootcamp/downloads/opt-350m/\"\n",
    "if os.path.isdir(local_model_path):\n",
    "    base_model_repo = local_model_path\n",
    "\n",
    "print(f\"Loading pretrained model and tokenizer from {base_model_repo}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_repo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of examples presented to the model in a given step.\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build training and evaluation batches\n",
    "\n",
    "Combining more than one rows of data into a \"batch\" helps increase throughput on GPUs, both for training and for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import PreTrainedTokenizerBase\n",
    "\n",
    "\n",
    "def build_data_batches(\n",
    "    dataset_rows: list[dict[str, Any]],\n",
    "    batch_size: int,\n",
    "    tokenizer: PreTrainedTokenizerBase,\n",
    "    text_column_name: str = \"text\",\n",
    "    label_column_name: str = \"label\",\n",
    ") -> list[dict[str, torch.TensorType]]:\n",
    "    \"\"\"Build batches out of a list of examples.\n",
    "\n",
    "    For simplicity, if some trailing examples don't fit in a batch,\n",
    "    those examples would not be included in the output.\n",
    "\n",
    "    Params\n",
    "    ------\n",
    "        dataset_rows: List of dictionaries, one for each row of the dataset.\n",
    "        batch_size: Number of examples to include in each batch.\n",
    "        tokenizer: HuggingFace Tokenizer\n",
    "        text_column_name: name of text column in dataset_rows\n",
    "        label_column_name: name of label column in dataset_rows\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        list of dictionaries, one for each batch.\n",
    "        Each dictionary consists of:\n",
    "        - input_ids: integer tensors of shape (batch_size, max_width)\n",
    "        - attention_mask: boolean tensor of same shape as input_ids,\n",
    "            highlighting which of the items in the input_ids tensor are paddings\n",
    "            (not actual words, but added for performance reasons)\n",
    "        - labels: tensor of shape (batch_size,) one per row.\n",
    "\n",
    "        Values of the dictionary are tensors.\n",
    "\n",
    "    \"\"\"\n",
    "    # list of batches. Text would be replaced with tokenization tensors,\n",
    "    # while labels would be stored as PyTorch Tensors (lists)\n",
    "    output: list[dict[str, torch.TensorType]] = []\n",
    "\n",
    "    # Buffer for a batch of dataset rows, not yet tokenized.\n",
    "    text_buffer: list[str] = []\n",
    "    label_buffer: list[Any] = []\n",
    "    num_examples_in_buffer = 0\n",
    "\n",
    "    for row in tqdm(dataset_rows):\n",
    "        # Add text from the dataset \"row\" to the text buffer.\n",
    "        # Recall that \"row\" is a dictionary mapping dataset feature\n",
    "        # to value. The name of the text feature is in the \"text_column_name\"\n",
    "        # variable.\n",
    "        #\n",
    "        # Hint:\n",
    "        #   text_buffer.append(...)\n",
    "        label_buffer.append(row[label_column_name])\n",
    "\n",
    "        num_examples_in_buffer += 1\n",
    "\n",
    "        # Group (batch_size) raw dataset rows into one processed batch of tensors.\n",
    "        if num_examples_in_buffer == batch_size:\n",
    "            assert len(text_buffer) == batch_size\n",
    "            assert len(label_buffer) == batch_size\n",
    "\n",
    "            output.append(\n",
    "                {\n",
    "                    **tokenizer(text_buffer, return_tensors=\"pt\", padding=True),\n",
    "                    \"labels\": torch.Tensor(label_buffer).type(torch.long),\n",
    "                },\n",
    "            )\n",
    "            text_buffer = []\n",
    "            label_buffer = []\n",
    "            num_examples_in_buffer = 0\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset_dict = {\n",
    "    split_name: build_data_batches(dataset_split, BATCH_SIZE, tokenizer)\n",
    "    for split_name, dataset_split in dataset_dict.items()\n",
    "}\n",
    "\n",
    "print(\n",
    "    \"processed batches:\",\n",
    "    {split_name: len(split) for split_name, split in processed_dataset_dict.items()},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_batch = processed_dataset_dict[\"train\"][0]\n",
    "print(\"Example batch:\", {k: v.shape for k, v in example_batch.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    model: torch.nn.Module, processed_dataset: list[dict[str, torch.Tensor]],\n",
    ") -> tuple[float, float]:\n",
    "    \"\"\"Evaluate model on given dataset.\n",
    "\n",
    "    See above for a demo of the inner working of this function.\n",
    "\n",
    "    Params:\n",
    "    -------\n",
    "        model: transformer classifier model to evaluate.\n",
    "        processed_dataset: list of pre-processed (tokenized) batches.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        (cross-entropy loss, accuracy)\n",
    "\n",
    "    \"\"\"\n",
    "    criteria = torch.nn.CrossEntropyLoss()\n",
    "    loss_values: list[torch.Tensor] = []\n",
    "    accuracy_values: list[torch.Tensor] = []\n",
    "\n",
    "    with tqdm(total=len(processed_dataset)) as progress_bar:\n",
    "        for batch_cpu in processed_dataset:\n",
    "            batch = {k: v.to(model.device) for k, v in batch_cpu.items()}\n",
    "            logits = model(\n",
    "                input_ids=batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "            ).logits\n",
    "            labels = batch[\"labels\"]\n",
    "\n",
    "            loss = criteria(logits, labels).item()\n",
    "            loss_values.append(loss)\n",
    "\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            accuracy = torch.mean((predictions == labels).type(torch.float)).item()\n",
    "            accuracy_values.append(accuracy)\n",
    "\n",
    "            progress_bar.update(1)\n",
    "\n",
    "        avg_loss = sum(loss_values) / len(loss_values)\n",
    "        avg_accuracy = sum(accuracy_values) / len(accuracy_values)\n",
    "\n",
    "        progress_bar.set_description(\n",
    "            f\"Eval loss {avg_loss:.3f} acc {avg_accuracy * 100:.1f}%\",\n",
    "        )\n",
    "\n",
    "    return avg_loss, avg_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "device = torch.device(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "train_data = processed_dataset_dict[\"train\"]\n",
    "validation_data = processed_dataset_dict[\"validation\"]\n",
    "\n",
    "print(\n",
    "    \"Adding a new classification layer on top of pretrained weights- \\n\"\n",
    "    \"you will see a reminder from HuggingFace that \"\n",
    "    '\"You should probably TRAIN this model\":',\n",
    ")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    base_model_repo, num_labels=num_label_classes,\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "criteria = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Train for 100 steps\n",
    "for train_step, batch_cpu in enumerate(tqdm(processed_dataset_dict[\"train\"][:100])):\n",
    "    # evaluate model on validation set every 10 steps\n",
    "    if train_step % 10 == 0:\n",
    "        model.eval()  # turn off back-propagation to evaluate faster\n",
    "        eval_accuracy, eval_loss = evaluate_model(model, validation_data)\n",
    "        model.train()  # turn back-propagation back on\n",
    "\n",
    "    # Send batch to accelerator device\n",
    "    batch = {k: v.to(device) for k, v in batch_cpu.items()}\n",
    "    model_output = model(\n",
    "        input_ids=batch[\"input_ids\"],\n",
    "        attention_mask=batch[\"attention_mask\"],\n",
    "    )\n",
    "\n",
    "    training_loss = criteria(model_output.logits, batch[\"labels\"])\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    training_loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "eval_loss, eval_accuracy = evaluate_model(model, validation_data)\n",
    "\n",
    "print(f\"Final validation loss: {eval_loss:.2f}; accuracy {eval_accuracy * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels for tweet_eval emotion:\n",
    "# 0: anger; 1: joy; 2: optimism; 3: sadness\n",
    "example_input = [\"Good book!\", \"Bad book!\", \"It is raining all day!\"]\n",
    "example_input_encoded = tokenizer(example_input, return_tensors=\"pt\", padding=True)\n",
    "model_output = model(\n",
    "    input_ids=example_input_encoded[\"input_ids\"].to(device),\n",
    "    attention_mask=example_input_encoded[\"attention_mask\"].to(device),\n",
    ")\n",
    "\n",
    "predictions = torch.argmax(model_output.logits, dim=-1)\n",
    "print(\"predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer Integration\n",
    "\n",
    "HuggingFace provides abstractions for common use cases- including sequence classification. \n",
    "\n",
    "Reproduced from [transformers/en/tasks/sequence_classification](https://huggingface.co/docs/transformers/en/tasks/sequence_classification) with simplifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "device = torch.device(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "print(\n",
    "    \"Adding a new classification layer on top of pretrained weights- \\n\"\n",
    "    \"you will see a reminder from HuggingFace that \"\n",
    "    '\"You should probably TRAIN this model\":',\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_repo)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    base_model_repo, num_labels=num_label_classes,\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "tokenized_dataset = dataset_dict.map(\n",
    "    lambda examples: tokenizer(\n",
    "        # The input to tokenizer() should be a list of text (list[str]).\n",
    "        # each `examples` is a dict[str, list[Any]], mapping the name\n",
    "        # of the column of the dataset to a list of values from that column.\n",
    "        # Which column should you tokenize?\n",
    "        #\n",
    "        # hint: `examples[\"name_of_column\"]`\n",
    "    ),\n",
    "    batched=True,\n",
    ")\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    # How would you obtain the predictions from the logits tensor?\n",
    "    # Hint: using numpy, take argmax over the dimension (-1).\n",
    "    #\n",
    "    # If \"logits\" is a 2D array of shape (batch, num_choices),\n",
    "    # Your \"predictions\" should be a 1D array of shape (batch,)\n",
    "    #\n",
    "    # predictions = ...\n",
    "\n",
    "    assert isinstance(predictions, (np.ndarray, torch.Tensor))\n",
    "    assert predictions.shape == (logits.shape[0],)\n",
    "\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../../scratch/supervised_finetuning/checkpoints\",\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=0.25,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=10,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = trainer.model\n",
    "model.eval()\n",
    "eval_loss, eval_accuracy = evaluate_model(model, validation_data)\n",
    "\n",
    "print(f\"Final validation loss: {eval_loss:.2f}; accuracy {eval_accuracy * 100:.1f}%\")\n",
    "\n",
    "# labels for tweet_eval emotion:\n",
    "# 0: anger; 1: joy; 2: optimism; 3: sadness\n",
    "example_input = [\"Good book!\", \"Bad book!\", \"It is raining all day!\"]\n",
    "example_input_encoded = tokenizer(example_input, return_tensors=\"pt\", padding=True)\n",
    "model_output = model(\n",
    "    input_ids=example_input_encoded[\"input_ids\"].to(device),\n",
    "    attention_mask=example_input_encoded[\"attention_mask\"].to(device),\n",
    ")\n",
    "\n",
    "predictions = torch.argmax(model_output.logits, dim=-1)\n",
    "print(\"predictions:\", predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ft-lab-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
