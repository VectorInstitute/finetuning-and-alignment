{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PEFT Tutorial\n",
    "*(A bulk of the material of this tutorial is taken from Sebastian Raschka's [Code Lora from Scratch](https://lightning.ai/lightning-ai/studios/code-lora-from-scratch).)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from functools import partial\n",
    "\n",
    "import lightning as L\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from custom_lightning_module import CustomLightningModule\n",
    "from datasets import load_dataset\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring Paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = \"../../data/imdb/\"\n",
    "SAVED_MODEL_DIR = \"/projects/fta_bootcamp/trained_models/peft_demo/\"\n",
    "OUTPUT_DIR = \"../../scratch/peft/\" # main directory of the the demo output\n",
    "CHECKPOINT_DIR = f\"{OUTPUT_DIR}checkpoints\" # where to save checkpoints\n",
    "MODEL_NAME = \"distilbert-base-uncased\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Custom LoRA Layer <a id=\"LoRA_Anchor\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ncv--YQFv3zM"
   },
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "\n",
    "class LoRALayer(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, rank, alpha):\n",
    "        super().__init__()\n",
    "        std_dev = 1 / torch.sqrt(torch.tensor(rank).float())\n",
    "        self.W_a = torch.nn.Parameter(torch.randn(in_dim, rank) * std_dev)\n",
    "        self.W_b = torch.nn.Parameter(torch.zeros(rank, out_dim))\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass ### TODO: TODO: implement the forward pass of lora ###\n",
    "\n",
    "\n",
    "class LinearWithLoRA(torch.nn.Module):\n",
    "    def __init__(self, linear, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.lora = LoRALayer(\n",
    "            linear.in_features, linear.out_features, rank, alpha,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass ### TODO: TODO: implement the forward pass of lora layer ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iKLiB8_Py7kT",
    "outputId": "24ac20cf-f044-45a5-d342-48c119e35091"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "# a simple linear layer with 10 inputs and 1 output\n",
    "# requires_grad=False makes it non-trainable\n",
    "with torch.no_grad():\n",
    "    linear_layer = torch.nn.Linear(10, 1)\n",
    "\n",
    "# a simple example input\n",
    "x = torch.rand((1, 10))\n",
    "\n",
    "linear_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KNB_hhGu0pyL",
    "outputId": "50d64706-93c0-400c-d8af-c304751991b3"
   },
   "outputs": [],
   "source": [
    "lora_layer = LinearWithLoRA(linear=linear_layer, rank=8, alpha=1)\n",
    "lora_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cnB2Q5Yg06oF",
    "outputId": "28a3dde6-a127-4d28-afa5-f4556db58f0e"
   },
   "outputs": [],
   "source": [
    "lora_layer.lora.W_b = torch.nn.Parameter(lora_layer.lora.W_b + 0.01 * x[0])\n",
    "lora_layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cI2eU3XG5NlV"
   },
   "source": [
    "## Loading the Dataset into DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W2UlNJPV7HCp"
   },
   "outputs": [],
   "source": [
    "imdb_dataset = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files={\n",
    "        \"train\": os.path.join(DATASET_DIR, \"train.csv\"),\n",
    "        \"validation\": os.path.join(DATASET_DIR, \"val.csv\"),\n",
    "        \"test\": os.path.join(DATASET_DIR, \"test.csv\"),\n",
    "    },\n",
    ")\n",
    "\n",
    "print(imdb_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pbcTZXBl7nXW",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "print(\"Tokenizer input max length:\", tokenizer.model_max_length)\n",
    "print(\"Tokenizer vocabulary size:\", tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prwgIhGT7rKr"
   },
   "outputs": [],
   "source": [
    "def tokenize_text(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True, padding=True)\n",
    "\n",
    "imdb_tokenized = imdb_dataset.map(tokenize_text, batched=True, batch_size=None)\n",
    "del imdb_dataset\n",
    "imdb_tokenized.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NYIXdGrh75jd"
   },
   "source": [
    "## Setting Up DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ZV9sxEZ74gw"
   },
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, dataset_dict, partition_key=\"train\"):\n",
    "        self.partition = dataset_dict[partition_key]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.partition[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.partition.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e8etLSmv8AKz"
   },
   "outputs": [],
   "source": [
    "train_dataset = IMDBDataset(imdb_tokenized, partition_key=\"train\")\n",
    "val_dataset = IMDBDataset(imdb_tokenized, partition_key=\"validation\")\n",
    "test_dataset = IMDBDataset(imdb_tokenized, partition_key=\"test\")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=12,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=12,\n",
    "    num_workers=4,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=12,\n",
    "    num_workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting Number of Trainable Parameters Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R7tPqkMu9QpM"
   },
   "source": [
    "## Finetunning Last Two Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OH-ZTf4I8FAm"
   },
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME, num_labels=2)\n",
    "\n",
    "print(f\"Total number of trainable parameters for the base model: {count_parameters(model):,}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fv3ssM808Te6"
   },
   "source": [
    "Freeze all the layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2hzpYG5x8brd"
   },
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X0lCrJJP8h4g"
   },
   "source": [
    "Unfreeze the last two layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B_R3AwvP9JH0"
   },
   "outputs": [],
   "source": [
    "for param in model.pre_classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "print(f\"Total number of trainable parameters: {count_parameters(model):,}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7j3LUEJH9VLa"
   },
   "outputs": [],
   "source": [
    "lightning_model = CustomLightningModule(model)\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        dirpath=CHECKPOINT_DIR,\n",
    "        filename=\"last_two\",\n",
    "        save_top_k=1, # save top 1 model\n",
    "        mode=\"max\",\n",
    "        monitor=\"val_acc\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "logger = CSVLogger(save_dir=\"logs/\", name=\"my-model\")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=3,\n",
    "    callbacks=callbacks,\n",
    "    accelerator=\"gpu\",\n",
    "    precision=\"16-mixed\",\n",
    "    devices=1,\n",
    "    logger=logger,\n",
    "    log_every_n_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ejzK98ii9kcU"
   },
   "outputs": [],
   "source": [
    "# Comment cell below if you don't want to go through the training process. You can just load a trained model in the next cell.\n",
    "\n",
    "start = time.time()\n",
    "trainer.fit(model=lightning_model,\n",
    "            train_dataloaders=train_loader,\n",
    "            val_dataloaders=val_loader)\n",
    "\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print(f\"Time elapsed {elapsed/60:.2f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from a saved model\n",
    "lightning_model = CustomLightningModule.load_from_checkpoint(checkpoint_path=\"/projects/fta_bootcamp/trained_models/peft_demo//last_two.ckpt\", model=model)\n",
    "\n",
    "# train_acc = trainer.validate(lightning_model, dataloaders=train_loader, verbose=False)\n",
    "# val_acc = trainer.validate(lightning_model, dataloaders=val_loader, verbose=False)\n",
    "test_acc = trainer.test(lightning_model, dataloaders=test_loader, verbose=False)\n",
    "\n",
    "# print(f\"Train acc: {train_acc[0]['val_acc']*100:2.2f}%\")\n",
    "# print(f\"Val acc:   {val_acc[0]['val_acc']*100:2.2f}%\")\n",
    "print(f\"Test acc:  {test_acc[0]['accuracy']*100:2.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enter LoRA!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME, num_labels=2)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use our [LoRA layer](#LoRA_Anchor) implementation from before. Here's our current model *before* adding LoRA layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's wrap the query and value layers of transformer blocks with LoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_adaptation_layer(model, adaptation_layer, lora_r, lora_alpha, config):\n",
    "    assign_lora = partial(adaptation_layer, rank=lora_r, alpha=lora_alpha)\n",
    "\n",
    "    for layer in model.distilbert.transformer.layer:\n",
    "        if config.get(\"lora_query\"):\n",
    "            pass ### TODO: TODO: look at the model architecture and and use assign_lora function ###\n",
    "        if config.get(\"lora_key\"):\n",
    "            pass ### TODO: TODO: look at the model architecture and and use assign_lora function ###\n",
    "        if config.get(\"lora_value\"):\n",
    "            pass ### TODO: TODO: look at the model architecture and and use assign_lora function ###\n",
    "        if config.get(\"lora_projection\"):\n",
    "            pass ### TODO: TODO: look at the model architecture and and use assign_lora function ###\n",
    "        if config.get(\"lora_mlp\"):\n",
    "            ### look at the model architecture and and use assign_lora function (make sure you apply to both linear layers in fnn) ###\n",
    "            pass\n",
    "    if config.get(\"lora_head\"):\n",
    "        ### look at the model architecture and and use assign_lora function. Apply to both pre_classifier and classifier layers) ###\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"lora_query\": True,\n",
    "    \"lora_key\": False,\n",
    "    \"lora_value\": True,\n",
    "    \"lora_projection\": False,\n",
    "    \"lora_mlp\": False,\n",
    "    \"lora_head\": False,\n",
    "}\n",
    "apply_adaptation_layer(model, adaptation_layer=LinearWithLoRA, lora_r=8, lora_alpha=16, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the model after the LoRA layers are added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if linear layers are frozen\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total number of trainable parameters: {count_parameters(model):,}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tune with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lightning_model = CustomLightningModule(model)\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        dirpath=CHECKPOINT_DIR,\n",
    "        filename=\"lora\",\n",
    "        save_top_k=1, # save top 1 model\n",
    "        mode=\"max\",\n",
    "        monitor=\"val_acc\",\n",
    "    ),\n",
    "]\n",
    "logger = CSVLogger(save_dir=\"logs/\", name=\"my-model\")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=3,\n",
    "    callbacks=callbacks,\n",
    "    accelerator=\"gpu\",\n",
    "    precision=\"16-mixed\",\n",
    "    devices=1,\n",
    "    logger=logger,\n",
    "    log_every_n_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment cell below if you don't want to go through the training process. You can just load a trained model in the next cell.\n",
    "\n",
    "start = time.time()\n",
    "trainer.fit(model=lightning_model,\n",
    "            train_dataloaders=train_loader,\n",
    "            val_dataloaders=val_loader)\n",
    "\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print(f\"Time elapsed {elapsed/60:.2f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from a saved model\n",
    "lightning_model = CustomLightningModule.load_from_checkpoint(checkpoint_path=\"/projects/fta_bootcamp/trained_models/peft_demo/lora.ckpt\", model=model)\n",
    "\n",
    "# train_acc = trainer.validate(lightning_model, dataloaders=train_loader, verbose=False)\n",
    "# val_acc = trainer.validate(lightning_model, dataloaders=val_loader, verbose=False)\n",
    "test_acc = trainer.test(lightning_model, dataloaders=test_loader, verbose=False)\n",
    "\n",
    "# print(f\"Train acc: {train_acc[0]['val_acc']*100:2.2f}%\")\n",
    "# print(f\"Val acc:   {val_acc[0]['val_acc']*100:2.2f}%\")\n",
    "print(f\"Test acc:  {test_acc[0]['accuracy']*100:2.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using HF's LoRA\n",
    "\n",
    "We can replace our custom LoRA implementation with an implementation from the [peft library](https://github.com/huggingface/peft). Peft is an open-source, one-stop-shop library from HuggingFace for *parameter efficient fine-tuning* (PEFT) and is integrated with the their [transformers library](https://github.com/huggingface/transformers) for easy model training and inference. \n",
    "\n",
    "Here's a sample snippet for how to prepare a model for PEFT training with LoRA. We can easily fine-tune the DistillBert model we had before with this implementation of the LoRA layer, instead of our custom layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_lin\", \"v_lin\"],\n",
    ")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's this [DoRA](https://arxiv.org/pdf/2402.09353) thing I keep hearing about?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME, num_labels=2)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code inspired by https://github.com/catid/dora/blob/main/dora.py\n",
    "class LinearWithDoRA(torch.nn.Module):\n",
    "    def __init__(self, linear, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.lora = LoRALayer(\n",
    "            linear.in_features, linear.out_features, rank, alpha,\n",
    "        )\n",
    "\n",
    "        self.m = nn.Parameter(\n",
    "            self.linear.weight.norm(p=2, dim=0, keepdim=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        lora = self.lora.W_a @ self.lora.W_b\n",
    "        combined_weight = self.linear.weight + self.lora.alpha*lora.T\n",
    "        column_norm = combined_weight.norm(p=2, dim=0, keepdim=True)\n",
    "        V = combined_weight / column_norm\n",
    "        new_weight = self.m * V\n",
    "        return F.linear(x, new_weight, self.linear.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"lora_query\": True,\n",
    "    \"lora_key\": False,\n",
    "    \"lora_value\": True,\n",
    "    \"lora_projection\": False,\n",
    "    \"lora_mlp\": False,\n",
    "    \"lora_head\": False,\n",
    "}\n",
    "apply_adaptation_layer(model, adaptation_layer=LinearWithDoRA, lora_r=8, lora_alpha=16, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total number of trainable parameters: {count_parameters(model):,}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune with DoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lightning_model = CustomLightningModule(model)\n",
    "\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        dirpath=\"\",\n",
    "        filename=\"dora\",\n",
    "        save_top_k=1, # save top 1 model\n",
    "        mode=\"max\",\n",
    "        monitor=\"val_acc\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "logger = CSVLogger(save_dir=\"logs/\", name=\"my-model\")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=3,\n",
    "    callbacks=callbacks,\n",
    "    accelerator=\"gpu\",\n",
    "    precision=\"16-mixed\",\n",
    "    devices=1,\n",
    "    logger=logger,\n",
    "    log_every_n_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment cell below if you don't want to go through the training process. You can just load a trained model in the next cell.\n",
    "\n",
    "start = time.time()\n",
    "trainer.fit(model=lightning_model,\n",
    "            train_dataloaders=train_loader,\n",
    "            val_dataloaders=val_loader)\n",
    "\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print(f\"Time elapsed {elapsed/60:.2f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from a saved model\n",
    "lightning_model = CustomLightningModule.load_from_checkpoint(checkpoint_path=\"/projects/fta_bootcamp/trained_models/peft_demo/dora.ckpt\", model=model)\n",
    "\n",
    "# train_acc = trainer.validate(lightning_model, dataloaders=train_loader, verbose=False)\n",
    "# val_acc = trainer.validate(lightning_model, dataloaders=val_loader, verbose=False)\n",
    "test_acc = trainer.test(lightning_model, dataloaders=test_loader, verbose=False)\n",
    "\n",
    "# print(f\"Train acc: {train_acc[0]['val_acc']*100:2.2f}%\")\n",
    "# print(f\"Val acc:   {val_acc[0]['val_acc']*100:2.2f}%\")\n",
    "print(f\"Test acc:  {test_acc[0]['accuracy']*100:2.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "finetune_demo",
   "language": "python",
   "name": "finetune_demo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
